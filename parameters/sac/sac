# config.toml
[general]
seed = 42
num_steps = 3000000
save_interval = 100000
save_dir = "ckpt"
log_interval = 1000

[environment]
frame_skip = 1
frame_stack = 1

[network]
hidden_dim = [512, 512, 256]
activation_fn = "relu"
kernel_init = "orthogonal"
final_layer_init = 0.003

[sac]
batch_size = 512
gamma = 0.99
lr = 3e-4
critic_lr = 3e-4
alpha_lr = 3e-4
buffer_size = 2000000
tau = 0.005
alpha = "auto"
updates_per_step = 1
start_steps = 20000
target_update_interval = 2
reward_scale = 5.0
max_grad_norm = 0.5
learning_starts = 10000
gradient_steps = 2
max_episodes = 1500

[evaluation]
eval_freq = 30000
eval_episodes = 15

[reward_shaping]
forward_reward_weight = 4.0
upright_reward_weight = 3.0
base_reward_weight = 1.0
energy_penalty_weight = 0.15
success_threshold = 300
bonus_reward = 3.0
success_multiplier = 1.5

[observation]
obs_clip_min = -5
obs_clip_max = 5
running_stats_momentum = 0.99

[priority_replay]
priority_exponent = 0.6
priority_epsilon = 1e-6

[lr_scheduler]
lr_scheduler_patience = 25
lr_scheduler_factor = 0.5
min_lr = 1e-6

[exploration]
noise_scale_start = 0.2
noise_decay_steps = 1500000
min_noise_scale = 0.02
